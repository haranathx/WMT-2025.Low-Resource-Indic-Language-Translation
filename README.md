# üåç WMT 2025 ‚Äì Low-Resource Indic Machine Translation (JU-NLP)

## üöÄ Project Overview

This repository contains our official submission to the **WMT 2025 Shared Task on Machine Translation**, presented in the paper:

**‚ÄúJU-NLP: Improving Low-Resource Indic Translation System with Efficient LoRA-Based Adaptation‚Äù**

The project focuses on improving **neural machine translation (NMT)** performance for **low-resource Indic languages** such as **Assamese, Manipuri, Mizo, and Bodo** by combining strong preprocessing strategies with **parameter-efficient fine-tuning (LoRA)** on large multilingual models.

Participation in WMT reflects applied research experience in **large-scale MT systems**, **multilingual NLP**, and **shared-task benchmarking**.

---

## üèÜ Task Description

**Conference:** WMT 2025 (Tenth Conference on Machine Translation)
**Track:** Shared Task ‚Äì Low-Resource Indic Machine Translation
**Domain:** Multilingual Neural Machine Translation (NMT)

The task addresses key challenges in Indic MT:

* Scarcity of parallel corpora
* Script and orthographic diversity
* Rich morphology
* Domain mismatch between training and test data

---

## üß† Proposed Approach

We propose a **unified multilingual NMT framework** that leverages both **large pre-trained models** and **efficient adaptation techniques**.

### üîπ Core Components

1. **Language-Specific Preprocessing**

   * Script normalization
   * Noise removal and filtering
   * Token-level consistency handling

2. **Pre-trained Backbone Models**

   * **NLLB-200** for multilingual translation
   * **mBART** as a strong encoder‚Äìdecoder baseline

3. **Parameter-Efficient Fine-Tuning**

   * **LoRA (Low-Rank Adaptation)** applied to attention layers
   * Reduces memory footprint while retaining performance

4. **Unified Multilingual Training**

   * Single model trained across multiple Indic languages
   * Improves cross-lingual transfer for extremely low-resource pairs

---

## ‚öôÔ∏è Training Details

* **Framework:** PyTorch, Hugging Face Transformers
* **Fine-Tuning Strategy:** LoRA-based adaptation
* **Optimization:** AdamW
* **Training Setup:** GPU-accelerated
* **Evaluation:** BLEU and shared-task evaluation metrics

The LoRA-based approach significantly lowers computational cost compared to full fine-tuning while maintaining competitive translation quality.

---

## üèÜ Leaderboard Achievements & Results

Based on the **official WMT 2025 Indic MT shared task leaderboard**, JU-NLP achieved **strong and competitive rankings** across multiple low-resource Indic language pairs among **17 participating teams worldwide**. ÓàÄfileciteÓàÇturn2file0ÓàÅ

### üîπ Key Achievements (JU-NLP)

* ‚úÖ **Participated in WMT 2025 Low-Resource Indic MT Shared Task**
* üåç Competed against **17 international teams** from academia and industry
* üèÖ Achieved **Top-3 ranking** in multiple language directions

### üî∏ Notable Language-wise Results

**Manipuri ‚Üí English (mni-en)**

* ü•á **Rank 1** out of all teams
* **BLEU:** 8.10 (Highest on leaderboard)

**English ‚Üí Manipuri (en-mni)**

* ü•à **Rank 2** overall
* **BLEU:** 4.12

**Mizo ‚Üí English (lus-en)**

* ü•á **Rank 1** overall
* **BLEU:** 12.30

**English ‚Üí Mizo (en-lus)**

* ü•á **Rank 1** overall
* **BLEU:** 15.83

**English ‚Üí Bodo (en-bodo)**

* ü•â **Rank 3** overall
* **BLEU:** 19.71

These results demonstrate the effectiveness of our **LoRA-based multilingual adaptation strategy**, particularly for **extremely low-resource Indic languages**.

---

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ data/               # Parallel corpora and processed datasets
‚îú‚îÄ‚îÄ preprocessing/      # Language-specific preprocessing scripts
‚îú‚îÄ‚îÄ src/                # Training and evaluation code
‚îú‚îÄ‚îÄ models/             # Fine-tuned MT checkpoints
‚îú‚îÄ‚îÄ results/            # Translation outputs and scores
‚îú‚îÄ‚îÄ requirements.txt    # Dependencies
‚îî‚îÄ‚îÄ README.md           # Project documentation
```

---

## üìå Key Contributions

* Applied **LoRA-based adaptation** to Indic MT at scale
* Built a **unified multilingual translation system**
* Addressed real-world challenges in **low-resource NLP**
* Validated approach in an **international shared task (WMT)**

---

## ‚ö†Ô∏è Limitations

* Performance still constrained by data scarcity
* Domain mismatch affects generalization
* Some languages require deeper linguistic normalization

---

## üîÆ Future Work

* Incorporate synthetic data generation (back-translation)
* Explore domain-adaptive fine-tuning
* Extend to additional Indic languages
* Investigate instruction-tuned MT models

---

## üéØ Why This Project Matters

This project demonstrates hands-on experience with:

* Large-scale **multilingual NLP systems**
* **Low-resource language processing**
* Parameter-efficient adaptation of foundation models
* Research-driven ML development

Relevant roles:

* Machine Learning Engineer
* NLP Engineer
* Research Engineer (NLP)

---

## üì¨ Contact
* **Haranath Mondal**
Data Analyst | Aspiring Data Scientist | NLP & ML Research Enthusiast

- **Final leaderboard:** https://drive.google.com/file/d/1Fh7Giu6kbMXnlRsleGgGsxRKiIYmrbgN/view
- **GitHub:** https://github.com/haranathx
- **LinkedIn:** https://www.linkedin.com/in/haranathmondal/  
- **Email:** haranathx@gmail.com
- **Portfolio:** https://haranathx.vercel.app

---

## üìÑ Paper Reference

If you use this work, please cite:

> JU-NLP: Improving Low-Resource Indic Translation System with Efficient LoRA-Based Adaptation.
> Proceedings of the Tenth Conference on Machine Translation (WMT 2025).

---

‚≠ê If you find this repository useful, consider starring it!
